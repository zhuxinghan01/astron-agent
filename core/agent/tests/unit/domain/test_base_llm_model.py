"""åŸºç¡€LLMæ¨¡å‹å•å…ƒæµ‹è¯•æ¨¡å—."""

import asyncio
from typing import Any, AsyncIterator, Dict, List
from unittest.mock import AsyncMock, Mock, patch

import pytest

from domain.models.base import BaseLLMModel


class TestBaseLLMModel:
    """BaseLLMModelæµ‹è¯•ç±»."""

    def setup_method(self) -> None:
        """æµ‹è¯•æ–¹æ³•åˆå§‹åŒ–."""
        # åˆ›å»ºmock AsyncOpenAIå®¢æˆ·ç«¯
        self.mock_llm = AsyncMock()  # pylint: disable=attribute-defined-outside-init
        self.model_name = "test_model"  # pylint: disable=attribute-defined-outside-init

        # ä½¿ç”¨model_constructç»•è¿‡PydanticéªŒè¯
        self.model = BaseLLMModel.model_construct(  # pylint: disable=attribute-defined-outside-init
            name=self.model_name, llm=self.mock_llm
        )

    def test_model_initialization(self) -> None:
        """æµ‹è¯•æ¨¡å‹åˆå§‹åŒ–."""
        assert self.model.name == self.model_name
        assert self.model.llm == self.mock_llm

    def test_model_config(self) -> None:
        """æµ‹è¯•æ¨¡å‹é…ç½®."""
        # éªŒè¯é…ç½®å…è®¸ä»»æ„ç±»å‹
        # éªŒè¯æ¨¡å‹é…ç½®å­˜åœ¨
        assert hasattr(BaseLLMModel, "model_config")

    @pytest.mark.asyncio
    async def test_create_completion_success(self) -> None:
        """æµ‹è¯•æˆåŠŸåˆ›å»ºcompletion."""
        test_messages = [{"role": "user", "content": "test message"}]
        test_stream = True
        expected_response = Mock()

        # Mock OpenAIå®¢æˆ·ç«¯è°ƒç”¨
        self.mock_llm.chat.completions.create = AsyncMock(
            return_value=expected_response
        )

        with patch("domain.models.base.agent_config") as mock_config:
            mock_config.default_llm_timeout = 60
            mock_config.default_llm_max_token = 10000

            result = await self.model.create_completion(test_messages, test_stream)

            # éªŒè¯ç»“æœ
            assert result == expected_response

            # éªŒè¯è°ƒç”¨å‚æ•°
            self.mock_llm.chat.completions.create.assert_called_once_with(
                messages=test_messages,
                stream=test_stream,
                model=self.model_name,
                timeout=60,
                max_tokens=10000,
            )

    @pytest.mark.asyncio
    @patch("domain.models.base.BaseLLMModel.create_completion", new_callable=AsyncMock)
    async def test_stream_success_without_span(self, mock_create: AsyncMock) -> None:
        """æµ‹è¯•æ— spançš„æµå¼å¤„ç†æˆåŠŸ."""
        test_messages = [{"role": "user", "content": "test message"}]
        test_stream = True

        # Mock chunkæ•°æ®
        mock_chunk1 = Mock()
        mock_chunk1.model_dump.return_value = {"code": 0, "content": "chunk1"}
        mock_chunk1.model_dump_json.return_value = '{"content": "chunk1"}'

        mock_chunk2 = Mock()
        mock_chunk2.model_dump.return_value = {"code": 0, "content": "chunk2"}
        mock_chunk2.model_dump_json.return_value = '{"content": "chunk2"}'

        # Mockå¼‚æ­¥è¿­ä»£å™¨
        async def mock_response_iterator() -> AsyncIterator[Mock]:
            yield mock_chunk1
            yield mock_chunk2

        mock_response = AsyncMock()
        mock_response.__aiter__ = AsyncMock(return_value=mock_response_iterator())

        mock_create.return_value = mock_response_iterator()

        # æ”¶é›†æµå¼ç»“æœ
        results = []
        async for chunk in self.model.stream(test_messages, test_stream, None):
            results.append(chunk)

        # éªŒè¯ç»“æœ
        assert len(results) == 2
        assert results[0] == mock_chunk1
        assert results[1] == mock_chunk2

    @pytest.mark.asyncio
    @patch("domain.models.base.BaseLLMModel.create_completion", new_callable=AsyncMock)
    async def test_stream_success_with_span(self, mock_create: AsyncMock) -> None:
        """æµ‹è¯•å¸¦spançš„æµå¼å¤„ç†æˆåŠŸ."""
        test_messages = [
            {"role": "user", "content": "test user message"},
            {"role": "assistant", "content": "test assistant message"},
        ]
        test_stream = True

        # Mock span
        mock_span = Mock()
        mock_span.add_info_events = Mock()

        # Mock chunkæ•°æ®
        mock_chunk = Mock()
        mock_chunk.model_dump.return_value = {"code": 0, "content": "chunk"}
        mock_chunk.model_dump_json.return_value = '{"content": "chunk"}'

        async def mock_response_iterator() -> AsyncIterator[Mock]:
            yield mock_chunk

        mock_create.return_value = mock_response_iterator()

        # æ‰§è¡Œæµå¼å¤„ç†
        results = []
        async for chunk in self.model.stream(test_messages, test_stream, mock_span):
            results.append(chunk)

        # éªŒè¯spanè°ƒç”¨
        assert (
            mock_span.add_info_events.call_count >= 4
        )  # messages + model + stream + chunk

        # éªŒè¯å…·ä½“çš„spanè°ƒç”¨
        calls = mock_span.add_info_events.call_args_list
        message_calls = [
            call for call in calls if "user" in str(call) or "assistant" in str(call)
        ]
        assert len(message_calls) >= 2

    @pytest.mark.asyncio
    @patch("domain.models.base.llm_plugin_error")
    @patch("domain.models.base.BaseLLMModel.create_completion", new_callable=AsyncMock)
    async def test_stream_error_chunk_handling(
        self, mock_create: AsyncMock, mock_error_handler: Mock
    ) -> None:
        """æµ‹è¯•é”™è¯¯chunkå¤„ç†."""
        test_messages = [{"role": "user", "content": "test message"}]
        test_stream = True

        # Mocké”™è¯¯chunk
        mock_error_chunk = Mock()
        mock_error_chunk.model_dump.return_value = {
            "code": 500,
            "message": "Internal server error",
        }

        async def mock_response_iterator() -> AsyncIterator[Mock]:
            yield mock_error_chunk

        mock_create.return_value = mock_response_iterator()

        # æ‰§è¡Œå¹¶éªŒè¯é”™è¯¯å¤„ç†
        results = []
        async for chunk in self.model.stream(test_messages, test_stream, None):
            results.append(chunk)

            # éªŒè¯é”™è¯¯å¤„ç†å™¨è¢«è°ƒç”¨
            mock_error_handler.assert_called_once_with(500, "Internal server error")

    @pytest.mark.asyncio
    @patch("domain.models.base.BaseLLMModel.create_completion", new_callable=AsyncMock)
    async def test_stream_api_timeout_error(self, mock_create: AsyncMock) -> None:
        """æµ‹è¯•APIè¶…æ—¶é”™è¯¯å¤„ç†."""
        test_messages = [{"role": "user", "content": "test message"}]
        test_stream = True

        # pylint: disable=import-outside-toplevel
        from openai import APITimeoutError

        # pylint: disable=import-outside-toplevel
        from exceptions.plugin_exc import PluginExc

        # åˆ›å»ºä¸€ä¸ªmockè¯·æ±‚å¯¹è±¡
        mock_request = Mock()
        mock_create.side_effect = APITimeoutError(mock_request)

        # éªŒè¯å¼‚å¸¸å¤„ç†
        with pytest.raises(PluginExc, match="è¯·æ±‚æœåŠ¡è¶…æ—¶"):
            results = []
            async for chunk in self.model.stream(test_messages, test_stream, None):
                results.append(chunk)

    @pytest.mark.asyncio
    @patch("domain.models.base.llm_plugin_error")
    @patch("domain.models.base.BaseLLMModel.create_completion", new_callable=AsyncMock)
    async def test_stream_api_error_handling(
        self, mock_create: AsyncMock, mock_error_handler: Mock
    ) -> None:
        """æµ‹è¯•APIé”™è¯¯å¤„ç†."""
        test_messages = [{"role": "user", "content": "test message"}]
        test_stream = True
        mock_span = Mock()
        mock_span.add_info_events = Mock()

        # pylint: disable=import-outside-toplevel
        from openai import APIError

        # Mock APIé”™è¯¯
        api_error = APIError(
            message="API Error", request=Mock(), body={"error": "Bad request"}
        )
        api_error.code = "400"

        mock_create.side_effect = api_error

        # æ‰§è¡Œå¹¶éªŒè¯é”™è¯¯å¤„ç†
        results = []
        async for chunk in self.model.stream(test_messages, test_stream, mock_span):
            results.append(chunk)

        # éªŒè¯spanè®°å½•é”™è¯¯ä¿¡æ¯
        assert mock_span.add_info_events.call_count >= 4

        # éªŒè¯é”™è¯¯å¤„ç†å™¨è¢«è°ƒç”¨
        mock_error_handler.assert_called_once_with("400", "API Error")

    @pytest.mark.asyncio
    @patch("domain.models.base.llm_plugin_error")
    @patch("domain.models.base.BaseLLMModel.create_completion", new_callable=AsyncMock)
    async def test_stream_value_error_handling(
        self, mock_create: AsyncMock, mock_error_handler: Mock
    ) -> None:
        """æµ‹è¯•å€¼é”™è¯¯å¤„ç†."""
        test_messages = [{"role": "user", "content": "test message"}]
        test_stream = True
        mock_span = Mock()
        mock_span.add_info_events = Mock()

        mock_create.side_effect = ValueError("Invalid value")

        # æ‰§è¡Œå¹¶éªŒè¯é”™è¯¯å¤„ç†
        results = []
        async for chunk in self.model.stream(test_messages, test_stream, mock_span):
            results.append(chunk)

        # éªŒè¯spanè®°å½•é”™è¯¯ä¿¡æ¯
        assert mock_span.add_info_events.call_count >= 3

        # éªŒè¯é”™è¯¯å¤„ç†å™¨è¢«è°ƒç”¨
        mock_error_handler.assert_called_once_with("-1", "Invalid value")

    @pytest.mark.asyncio
    @patch("domain.models.base.BaseLLMModel.create_completion", new_callable=AsyncMock)
    async def test_stream_concurrent_access(self, mock_create: AsyncMock) -> None:
        """æµ‹è¯•å¹¶å‘è®¿é—®æµå¼å¤„ç†."""
        test_messages = [{"role": "user", "content": "test message"}]

        # Mock chunkæ•°æ®
        mock_chunk = Mock()
        mock_chunk.model_dump.return_value = {"code": 0, "content": "chunk"}
        mock_chunk.model_dump_json.return_value = '{"content": "chunk"}'

        def mock_response_iterator_factory() -> AsyncIterator[Mock]:
            async def mock_response_iterator() -> AsyncIterator[Mock]:
                await asyncio.sleep(0.01)  # æ¨¡æ‹Ÿå»¶è¿Ÿ
                yield mock_chunk

            return mock_response_iterator()

        mock_create.side_effect = (
            lambda *_args, **_kwargs: mock_response_iterator_factory()
        )

        # åˆ›å»ºå¹¶å‘ä»»åŠ¡
        tasks = []
        for _ in range(3):
            task = asyncio.create_task(
                self._collect_stream_results(test_messages, True)
            )
            tasks.append(task)

        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        results = await asyncio.gather(*tasks)

        # éªŒè¯æ‰€æœ‰ä»»åŠ¡éƒ½æˆåŠŸ
        for result_list in results:
            assert len(result_list) == 1
            assert result_list[0] == mock_chunk

    async def _collect_stream_results(
        self, messages: List[Dict[str, Any]], stream: bool
    ) -> List[Any]:
        """è¾…åŠ©æ–¹æ³•ï¼šæ”¶é›†æµå¼ç»“æœ."""
        results = []
        async for chunk in self.model.stream(messages, stream, None):
            results.append(chunk)
        return results

    @pytest.mark.asyncio
    @patch("domain.models.base.BaseLLMModel.create_completion", new_callable=AsyncMock)
    async def test_unicode_message_handling(self, mock_create: AsyncMock) -> None:
        """æµ‹è¯•Unicodeæ¶ˆæ¯å¤„ç†."""
        unicode_messages = [
            {"role": "user", "content": "æµ‹è¯•ä¸­æ–‡æ¶ˆæ¯ğŸš€"},
            {"role": "assistant", "content": "ä¸­æ–‡å›å¤âœ…"},
        ]
        test_stream = True
        mock_span = Mock()
        mock_span.add_info_events = Mock()

        # Mock chunkæ•°æ®
        mock_chunk = Mock()
        mock_chunk.model_dump.return_value = {"code": 0, "content": "ä¸­æ–‡å“åº”"}
        mock_chunk.model_dump_json.return_value = '{"content": "ä¸­æ–‡å“åº”"}'

        async def mock_response_iterator() -> AsyncIterator[Mock]:
            yield mock_chunk

        mock_create.return_value = mock_response_iterator()

        # æ‰§è¡Œæµå¼å¤„ç†
        results = []
        async for chunk in self.model.stream(unicode_messages, test_stream, mock_span):
            results.append(chunk)

        # éªŒè¯Unicodeå†…å®¹æ­£ç¡®å¤„ç†
        assert len(results) == 1
        assert results[0] == mock_chunk

        # éªŒè¯spanè®°å½•äº†Unicodeå†…å®¹
        calls = mock_span.add_info_events.call_args_list
        unicode_calls = [
            call for call in calls if any("æµ‹è¯•ä¸­æ–‡" in str(arg) for arg in call.args)
        ]
        assert len(unicode_calls) > 0

    @pytest.mark.asyncio
    @patch("domain.models.base.BaseLLMModel.create_completion", new_callable=AsyncMock)
    async def test_empty_messages_handling(self, mock_create: AsyncMock) -> None:
        """æµ‹è¯•ç©ºæ¶ˆæ¯åˆ—è¡¨å¤„ç†."""
        empty_messages: List[Dict[str, Any]] = []
        test_stream = True

        async def mock_empty_response() -> AsyncIterator[Any]:
            # ç©ºçš„å¼‚æ­¥ç”Ÿæˆå™¨ï¼Œç”¨äºæ¨¡æ‹Ÿæ— å“åº”æƒ…å†µ
            return
            yield  # pylint: disable=unreachable

        mock_create.return_value = mock_empty_response()

        # æ‰§è¡Œæµå¼å¤„ç†
        results = []
        async for chunk in self.model.stream(empty_messages, test_stream, None):
            results.append(chunk)

        # éªŒè¯ç©ºæ¶ˆæ¯å¤„ç†
        assert len(results) == 0

    def test_model_attribute_access(self) -> None:
        """æµ‹è¯•æ¨¡å‹å±æ€§è®¿é—®."""
        # æµ‹è¯•åç§°è®¿é—®
        assert self.model.name == self.model_name

        # æµ‹è¯•LLMå®¢æˆ·ç«¯è®¿é—®
        assert self.model.llm == self.mock_llm

        # æµ‹è¯•å±æ€§è®¾ç½®
        new_name = "new_model_name"
        self.model.name = new_name
        assert self.model.name == new_name
